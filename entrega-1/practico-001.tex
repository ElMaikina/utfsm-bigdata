
\documentclass[9pt,letterpaper,twoside]{article}

\input{../latex-report-001/preamble}

% Datos de la asignatura
\institution{utfsm}
\classcode{INF356}
\classname{Computación Distribuida para Big Data}
\classsemester{2025-1}
\classparallel{200}

% Datos de la entrega
\doctitle{Trabajo Práctico 1}
\version{v1.0}

% Estudiante
\astudentname{Miguel}
\astudentlastname{Soto}
\astudentrol{20430363-0}
\astudentemail{miguel.sotod@sansano.usm.cl}
  
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Borrar o comentar esta sección instrucciones antes de entregar %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Despligue del cluster}

\subsection{Implementación del tutorial}

\noindent
Para la creacion del Cluster en Amazon Web Services, fue necesario seguir los pasos indicados en el tutorial
entregado en el Slide T01 - Hadoop Cluster - v1.0 entregado en Aula. El instructivo constaba de los siguientes
pasos:
\begin{itemize}
    \item Crear la cuenta de AWS
    \item Preparar en entorno del Cluster
    \item Configurar la maquina Master
    \item Configurar todos los Workers
    \item Inicializar el Cluster
    \item Ejecutar el script de Map Reduce con Hadoop
\end{itemize}

\subsubsection*{Crear la cuenta de AWS}

\noindent
Para la creacion de la cuenta de AWS simplemente segui las indicaciones en el correo de invitacion indicado por el profesor.
Posterior a eso, me fui directamente a la seccion de Modules, en donde estaba el Modulo de Launch AWS Academy Learner Lab,
que sirve para entrar a la Console Home de AWS.

\subsubsection*{Preparar en entorno del Cluster}

\noindent
Una vez en Console Home, creamos una instancia de tipo EC2, para la cual creamos los Key Pairs, el Security Group y la instancia
Master.

\noindent
Para los Key Pairs, creamos una llave de tipo RSA con formato de archivo PEM.

\noindent
Para el Security Group, creamos tres reglas, 
una Inbound Rule de tipo SSH con tipo de fuente Anywhere-IPv4
para entrar por SSH,
una Inbound Rule de tipo Custom TCP con rango de puerto 8088 y tipo de fuente Anywhere-IPv4
para la consola web de Hadoop,
una Inbound Rule de tipo Custom TCP con rango de puerto 9870 y tipo de fuente Anywhere-IPv4
para la consola web de HDFS
y finalmente, una Inbound Rule de tipo Custom TCP con rango de puerto de 0 a 65535 y tipo de 
fuente Anywhere-IPv4.

\noindent
Para la instancia Master creamos un nodo con Ubuntu 24.04 de tipo t2.micro y con 16gb de almacenamiento.
Esta instancia usara la Key y Security Group recien creados (al igual que las instancias Worker mas adelante). 
Una vez configurados los parametros, lanzamos la instancia y nos conectamos por SSH:

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Conectarse por primera vez al Master usando la llave PEM con la IP publica}, label={code/ssh_into_master.sh}]{code/ssh_into_master.sh}
\end{code}

\subsubsection*{Configurar la maquina Master}

\noindent
El primer paso es logicamente actualizar los paquetes del sistema, para esto es necesario ejecutar el siguiente
comando:

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Actualizar repositorios locales y descargar actualizaciones}, label={code/update.sh}]{code/update.sh}
\end{code}

\noindent
Una vez hecho esto, instalamos el paquete correspondiente a Java / OpenJDK 11:

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Descargar Java / OpenJDK 11 usando APT}, label={code/install_java.sh}]{code/install_java.sh}
\end{code}

\noindent
Una vez que hayamos resuelto todos los paquetes necesarios, tenemos que crear una llave publica con el siguiente
comando:

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Crear una llave publica en formato RSA y autorizarla}, label={code/create_pub_key.sh}]{code/create_pub_key.sh}
\end{code}

\noindent
Esta llave publica la guardaremos en la carpeta de Authorized Keys para SSH, lo cual permitira al Master
conectarse a si mismo por LocalHost.

\noindent
Descargamos Hadoop 3.3.6 desde la pagina oficial usando el comando WGET, el cual hace una request a la pagina para descargar el archivo comprimido en formato
TAR GZ, el cual tendremos que descomprimir y modificar para indicarle parametros especiales. Ejecutamos entonces los siguientes comandos:

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Descargar y descomprimir Hadoop en el Master}, label={code/download_hadoop.sh}]{code/download_hadoop.sh}
\end{code}

\noindent
Esto creara una carpeta llamada hadoop-3.3.6 en el directorio Home del Master, esta carpeta albergara los binarios yu scripts ejecutables para desplegar nuestro Distributed File System (DFS).
Para poder ejecutar los comandos en dicha carpeta de manera directa, modificaremos las Variables de Entorno de la maquina Master, de esta manera podremos ejecutar los comandos en cualquier
directorio. Dichas Variables tambien contienen rutas para la ejecucion de OpenJDK y la ubicacion de nuestra carpeta Hadoop, las cuales debemos obedecer para que funcione correctamente nuestro
cluster. Las variables de entorno en Linux se guardan en el directorio /etc/environment y solo puede modificarse por el Super Usuario (Root). Personalmente, a mi me gusta usar Vim, pero para
aquellas personas con gustos inferiores existen alternativas:

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Editar las Variables de Entorno usando el usuario Root y el editor de texto Vim}, label={code/edit_env_vars.sh}]{code/edit_env_vars.sh}
\end{code}

\noindent
Las Variables debiesen quedar asi:

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Variables de Entorno que usaremos para el cluster}, label={code/env_vars.sh}]{code/env_vars.sh}
\end{code}

\newpage

\noindent
Lo ultimo es modificar los archivos de configuracion de Hadoop en base a lo indicado por el profesor. Estos archivos luego seran enviados a los Workers cuando comprimamos la carpeta hadoop-3.3.6.

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={hadoop-3.3.6/etc/hadoop/core-site.xml}, label={code/core-site.xml}]{code/core-site.xml}
\end{code}

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={hadoop-3.3.6/etc/hadoop/hdfs-site.xml}, label={code/hdfs-site.xml}]{code/hdfs-site.xml}
\end{code}

\newpage

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={hadoop-3.3.6/etc/hadoop/mapred-site.xml}, label={code/mapred-site.xml}]{code/mapred-site.xml}
\end{code}

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={hadoop-3.3.6/etc/hadoop/yarn-site.xml}, label={code/yarn-site.xml}]{code/yarn-site.xml}
\end{code}

\newpage

\subsubsection*{Configurar todos los Workers}

\noindent
Los Workers tambien seran maquinas con Ubuntu 24.04 y de tipo t2.micro, usaran las mismas Key Pairs y el mismo Security Group.
Lo unico que cambiara entre ellos y el Master sera el almacenamiento, las cuales constaran solo de 8gb en total.
Una vez creados los Workers desde el interfaz de EC2 de AWS, tendremos que obtener sus IP's publicas y privadas. La primera
para conectarnos por primera vez, la segunda para establecer una conexion permanente.
Anotamos las IP's privadas de la siguiente manera:

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={IP's privadas de los Workers en la carpeta Home del Master}, label={code/worker_priv_ip.sh}]{code/worker_priv_ip.sh}
\end{code}

\noindent
Luego de tener esto, borramos el archivo comprimido de Hadoop que descargamos inicialmente y comprimimos el directorio que modificamos en el paso anterior para
luego enviarselo a los Workers. Estos luego descomprimiran el archivo de manera local para asi no abusar de nuestro ancho de banda limitado:

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Borrar el archivo hadoop-3.3.6.tar.gz y comprimimos el directorio hadoop-3.3.6}, label={code/arch_hadoop.sh}]{code/arch_hadoop.sh}
\end{code}

\noindent
Y como la tarea de configurar cada Worker manualmente es tedioso, usamos un script de Bash que realiza el trabajo por nosotros. El cual itera los siguientes pasos:

\noindent
Para cada una de las instancias Worker proceder de la siguiente forma:

\begin{enumerate}
\item Instalar la llave pública de la máquina Master en el Worker
\item Crear una llave para el Worker
\item Instalar la llave del Worker en el Master
\item Instalar la llave del Worker en el Worker
\item Actualizar el sistema operativo en el Worker
\item Instalar Java en el Worker (openjdk-11-jdk)
\item Editar el archivo /etc/environment de la misma forma que para Master
\item Reiniciar el worker
\item Transferir el archivo de despliegue de Hadoop de la máquina Master al Worker
\item Expandir el archivo de despliegue en la máquina Worker
\end{enumerate}

\noindent
El script utilizado quedo de la siguiente formula:

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Script para configurar todos los Workers desde el Master}, label={code/install1.sh}]{code/install1.sh}
\end{code}

\newpage

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Script para configurar todos los Workers desde el Master}, label={code/install2.sh}]{code/install2.sh}
\end{code}

Una vez configurado todo, reiniciamos el nodo Master y reingresamos cuando la maquina vuelva a estar activa.

\newpage

\subsubsection*{Inicializar el Cluster}

\noindent
Si las maquinas estan correctamente configuradas, podremos levantar el File System distribuido sin problemas.
Para esto es necesario formatear el disco e iniciar los servicios para la distribucion de este y para la gestion
de recursos entre los Workers. Cabe notar que por disco me refiero a unidad de almacenamiento, como vimos en
clases, un DFS no almacena datos de forma continua, si no que la reparte entre varios Nodos.

\noindent
Dicho esto, ejecutamos la siguiente serie de comandos:

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Formateamos el File System, lo iniciamos y llamamos al negociador de recursos YARN}, label={code/init_cluster.sh}]{code/init_cluster.sh}
\end{code}

\noindent
Luego, para corroborar de que todo esta en orden, ejecutamos el siguiente comando:

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Corroborar estado de nuestro HDFS}, label={code/hdfs-blocks1.sh}]{code/hdfs-blocks1.sh}
\end{code}

\noindent
La salida debiese ser algo del siguiente estilo:

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Estado del HDFS luego de haber ejecutado un Map Reduce}, label={code/hdfs-blocks2.sh}]{code/hdfs-blocks2.sh}
\end{code}

Notaremos que existe una seccion que muestra la cantidad de Data Nodes, si muestra 4 entonces configuramos correctamente
nuestro cluster.

\newpage

\subsubsection*{Ejecutar el script de Map Reduce con Hadoop}

Finalmente, ejecutamos nuestro algoritmo de Map Reduce. Para esto basta con crear un directorio /data, almacenar nuestro archivo
a leer y finalmente ejecutar el ejecutable JAR contenido en nuestra carpeta de Hadoop. Una vez hecho esto, podemos analizar la
salida del Map Reduce haciendole un Cat y pasandolo por Pipe a More para desplezgar la informacion por pantalla:

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Ejecutar el Map Reduce y leer su salida}, label={code/exec_map_reduce.sh}]{code/exec_map_reduce.sh}
\end{code}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{5-creacion_de_worker-3.PNG}
    \caption{Captura de pantalla de la ``AWS Management Console - EC2'' que muestra las máquinas del cluster creado con el tutorial}
    \label{5-creacion_de_worker-3.PNG}
\end{figure}

\newpage

\subsection{Expansión del cluster}

\noindent
Para expandir el cluster a 8 Workers, lo primero seria replicar las instancias desde AWS, estas debiesen tener los mismos
atributos que los workers originales. Lo segundo seria actualizar la configuracion desde el Master para incluir las nuevas
IP's y habilitar el uso de 4 Nodos desde los XML de Hadoop. Lo tercero seria configurar las conexiones de SSH hacia los workers
nuevos, para esto podemos reutilizar el script de Bash pero solo con las IP's privadas nuevas.

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Los archivos que hay que modificar antes de ejecutar el script de instalacion nuevamente}, label={code/expand_to_8.sh}]{code/expand_to_8.sh}
\end{code}

\begin{code}[H]
\lstinputlisting[style=bashstyle, caption={Código utilizado en la expansión del cluster}, label={lst:002}]{code/code-000.sh}
\end{code}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{expanded_cluster_ec2}
    \caption{Captura de pantalla de la ``AWS Management Console - EC2'' que muestra las máquinas del cluster expandido}
    \label{expanded_cluster_ec2}
\end{figure}

\subsection{Cliente web}

Aqui podemos apreciar todas las instancias desde la interfaz de EC2 de AWS, las instancias expandidas tienen el nombre de Worker
y un numoro que va desde 5 hasta 8. Sus direcciones IP privadas aparecen en el apartado de expansion y tienen las mismas
caracteristicas que los otros Nodos.


La figura \ref{hadoop.PNG} muestra la consola web de Hadoop y la figura \ref{hdfs.PNG} la consola web del HDFS.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{hadoop.PNG}
    \caption{Captura de pantalla del cliente web de Hadoop}
    \label{hadoop.PNG}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{hdfs.PNG}
    \caption{Captura de pantalla del cliente web de HDFS}
    \label{hdfs.PNG}
\end{figure}

\section{Instalación de Apache Hive}

\subsection{Procedimiento}

{\color{red} Desarrolle un procedimiento para instalar \textbf{Apache Hive} en su cluster. Utilice la versión 4.0.1. Tenga en consideración:
\begin{itemize}
    \item Las máquinas t2.micro son muy limitadas para levantar el servicio de Hive. Para esta sección se sugiere subir la máquina maestra a tipo \textbf{t3.medium} y los trabajadores a \textbf{t3.small}. Para cambiar el tipo de máquina no es necesario volver a desplegarla, basta con detener la máquina, cambiar su tipo desde la consola, y volver a iniciarla.
    \item Lograr la configuración correcta para Hive es un procedimiento que requiere bastante conocimiento y pruebas. El archivo zip de las instrucciones incluye el archivo \textbf{hive-example.xml} con la configuración a utilizar.
\end{itemize}}

{\color{red} En esta sección indique el procedimiento desarrollado y agregue cualquier código que haya sido utilizado para instalar y probar Apache Hive. Incluya el documento de configuración utilizado y explique cada uno de los parámetros definidos en este.}

\subsection{Prueba}

{\color{red} Para probar que la instalación de Hive funciona correctamente, puede utilizar el procedimiento disponible en el fragmento de código \ref{lst:004}. Este ejemplo asume que el archivo utilizado para probar el cluster \textbf{sw-script-e04.txt} se encuentra disponible en el DFS. El resultado esperado para la prueba indicada se entrega en el fragmento \ref{lst:005}}

\begin{code}[H]
\lstinputlisting[style=plainstyle, caption={Ejemplo de uso de Apache Hive}, label={lst:004}]{code/code-004.hql}
\end{code}

\begin{code}[H]
\lstinputlisting[style=plainstyle, caption={Resultado esperado para ejemplo de uso de Apache Hive {\color{red} Algunos de los valores han sido alterados, deben ser corregidos con el resultado de su procedimiento.}}, label={lst:005}]{code/code-005.txt}
\end{code}

\section{Exploración del HDFS}

{\color{red} Desarrolle y explique un script que utilizando bash permita obtener la lista de bloques en las que está guardado un archivo en el DFS, incluyendo las direcciones IP privadas de las máquinas que guardan cada copia del bloque. La salida del script se debe ver como lo indicado en el fragmento \ref{lst:006}.}

\begin{code}[H]
\lstinputlisting[style=plainstyle, caption={Ejemplo de uso de Apache Hive}, label={lst:006}]{code/code-006.txt}
\end{code}

{\color{red} Muestre el script desarrollado en el Código \ref{lst:007} y explique cada una de las partes del script.}

\begin{code}[H]
    \lstinputlisting[style=bashstyle, caption={Script de reporte de DFS}, label={lst:007}]{code/code-000.sh}
    \end{code}

{\color{red} En esta sección indique el procedimiento desarrollado y agregue cualquier código que haya sido utilizado para desarrollar el script.}
{\color{red} Utilizando la AWS-CLI, descargue en el nodo maestro del cluster el archivo \url{s3://utfsm-inf356-dataset/vlt_observations_000.csv}}\footnote{Este archivo es público y está guardado en un bucket S3, por lo que debe utilizar la opcion \textbf{--no-sign-request}. Este archivo pesa 371.1[MB] y es un archivo \textbf{csv} como el mismo formato al descargar \url{https://archive.eso.org/eso/eso_archive_main.html} con todos los campos. Una vez descargado, coloque el archivo en la carpeta \textbf{data} del DFS y muestre el resultado del script desarrollado previamente.}

\section{Uso del cluster}

\subsection{Importación}

{\color{red} Desarrolle un procedimiento que permita importar el archivo \textbf{vlt\_observations\_000.csv} a Hive desde el DFS respetando las columnas del archivo. Indique el código utilizado.}

\subsection{Parsing}

{\color{red} Desarrolle una propuesta para asignar un tipo apropiado a los datos importados y desarrolle un procedimiento que permita dar este formato creando una nueva tabla. Indique el código utilizado.}

\subsection{Análisis}

{\color{red} Piense 3 métricas sobre los datos interpretados, describa cada una de estas métricas y provea el código para obtener el resultado. Las métricas deben tener como mínimo 2 elementos de análisis (agrupamiento, contar, promedio, etc.). Ejemplos de métricas posibles son:
\begin{itemize}
  \item Cantidad de observaciones por cada tipo de observación (agrupa y cuenta)
  \item Ángulo promedio de declination de las observaciones del set para cada instrumento (agrupa y promedia)
  \item Seeing promedio por hora de observación (agrupa y promedia)
\end{itemize}
Provea un análisis sobre el desempeño del cluster al realizar estas operaciones. Incluya mediciones como tiempo de cómputo, máquinas usadas, cantidad de trabajos, cantidad de mappers y reducers, etc.}

\end{document}
