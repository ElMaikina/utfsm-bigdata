
\documentclass[12pt,letterpaper,twoside]{article}

\input{../latex-report-001/preamble}

% Datos de la asignatura
\institution{utfsm}
\classcode{INF356}
\classname{Computación Distribuida para Big Data}
\classsemester{2025-1}
\classparallel{200}

% Datos de la entrega
\doctitle{Trabajo Práctico 2}
\version{v1.0}

% Estudiante
\astudentname{Miguel}
\astudentlastname{Soto}
\astudentrol{201973623-K}
\astudentemail{miguel.sotod@sansano.usm.cl}
  
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Borrar o comentar esta sección instrucciones antes de entregar %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Map Reduce}

\noindent
Esta entrega consistia en modificar el codigo fuente de Hadoop para agregar funcionalidad extra, en donde habia que implementar metodos a partir de la funcion WordCount.

\noindent
Se pedia que crear una funcion que permitiese contar palabras previamente filtradas a partir de reglas indicadas en el informe.

\noindent
Luego, habia que crear otra funcion que permitiese extraer columnas de la salida de WordCount a partir del indice de la columna.

\noindent
Por motivos de eficiencia y por simplicidad, opte por parchar el codigo binario de Hadoop con las clases nuevas creadas a partir de WordCount en las funciones
de ejemplo originales, de esta forma pude invocarlas facilmente y no tener que compilar el codigo fuente cada vez que hiciera un cambio en el codigo.

\noindent
Grabe un video explicativo mostrando el proceso en ejecucion junto a las salidas de ambos programas, ya que en el practico inicial fue recurrente el tema de que no
dejar evidencia clara de los puntos que iba desarrollando. Aqui deje el {\color{blue}\href{https://www.youtube.com/watch?v=UVJUVDU4luk}{enlace}} al video en YouTube.

\newpage

\subsection{Implementación extendida de wordcounter}

\noindent
Para desarrollar esta nueva funcionalidad me base en la implementacion de la clase original de WordCount, la cual obtuve desde el repositorio publico de Apache Hadoop
en GitHub ({\color{blue}\href{https://github.com/apache/hadoop}{enlace}}), en donde dicha funcion se encuentra en la carpeta de ejemplos.

\noindent
La forma en la que luego parche el codigo, fue aprovechandome de los .jar que contiene el binario de Hadoop, estos ultimos son esencialmente
archivos comprimidos que contienen los archivos .class de todas las clases .java de la fuente. Mas aun, el comando jar tiene la funcionalidad
de actualizar un .jar con clases nuevas o con modificaciones de estas. Esto se puede lograr de la siguiente manera:

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{oracle-jar.PNG}
    \caption{Documentacion oficial de Oracle sobre como parchar un JAR}
    \label{hdfs.PNG}
\end{figure}

\noindent
A continuacion dejare el codigo original de WordCount y el codigo de ExtendedWordCount, hecho a partir del anterior.

\newpage

\begin{code}[H]
    \lstinputlisting[style=javastyle, caption={Codigo original de WordCount}, label={lst:002}]{code/WordCount.java}
\end{code}

\newpage

\begin{code}[H]
    \lstinputlisting[style=javastyle, caption={Codigo de ExtendedWordCount}, label={lst:002}]{code/ExtendedWordCount.java}
\end{code}

\newpage

\subsection{Implementación de selector de columna}

De la misma manera que el punto anterior, me base en WordCount para crear el codigo para la clase SelectColumnExtractor, el cual permite sustraer
una columna de la salida de WordCount en base al indice/numero de columna. Esta funcion en particular ejecuta WordCount si lo filtra al terminar
el proceso de Map-Reduce.

\newpage

\begin{code}[H]
    \lstinputlisting[style=javastyle, caption={Codigo de SelectColumnExtractor}, label={lst:002}]{code/SelectColumnExtractor.java}
\end{code}

\newpage

\subsection{Tiempo de ejecución}

\noindent
A continuación se muestran las medidas obtenidas utilizando el comando time en ExtendedWordCount:

\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Archivo} & \textbf{real (s)} & \textbf{user (s)} & \textbf{sys (s)} \\
\midrule
sw-script-e04.txt  & 0m32.822s  & 0m7.653s & 0m0.388s
\bottomrule
\end{tabular}
\caption{Tiempos de ejecución del WordCount extendido}
\end{table}

\noindent
Luego se muestran las medidas obtenidas utilizando el comando time en SelectColumnExtractor:

\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Archivo} & \textbf{real (s)} & \textbf{user (s)} & \textbf{sys (s)} \\
\midrule
sw-script-e04.txt  & 0m17.656s  & 0m7.137s & 0m0.350s
\bottomrule
\end{tabular}
\caption{Tiempos de ejecución del SelectColumnExtractor}
\end{table}

\noindent
Comparando ambos resultados, puedo concluir que:
\begin{itemize}
    \item El algoritmo de ExtendedWordCount es más intensivo en CPU, debido al preprocesamiento de texto, pero se mantiene eficiente gracias a la simplicidad del flujo MapReduce.
    \item El SelectColumnExtractor es más ligero y orientado a E/S, siendo más rápido en archivos simples, pero ligeramente mas costoso si las líneas contienen muchas columnas.
    \item Ambas soluciones escalan correctamente, pero el SelectColumnExtractor presenta tiempos de respuesta mas bajos en tareas estructuradas y simples.
\end{itemize}

\noindent
Los resultados se pueden ver en tiempo real en el video indicado al inicio del practico ({\color{blue}\href{https://github.com/apache/hadoop}{enlace}}).

\newpage

\section{Spark}

\subsection{Uso de training-bigdata-002}

{\color{red} Demuestre que ha logrado crear un cluster de spark, testear el cluster y destruir el cluster de acuerdo a las instrucciones entregadas en \url{https://github.com/ptoledo-teaching/training-bigdata-002}}

\subsection{Escalamiento vertical y horizontal}

{\color{red} Modificando el archivo de configuración de deployment del cluster de Spark, debe desplegar 5 configuraciones para medir el impacto que tienen el escalamiento vertical y horizonal en el tiempo de procesamiento. A cada estudiante le corresponderá una serie de configuraciones diferentes que se puede obtener de la siguiente lista:

    \begin{itemize}
        \item \textbf{19500967-8}: 2 x t3.large, 4 x t3.small, 8 x t3.large, 8 x t3.large, 8 x t3.small, 4 x t3.medium, 6 x t3.micro, 8 x t3.large, 2 x t3.micro, 4 x t3.medium
        \item \textbf{20068377-3}: 6 x t3.micro, 6 x t3.micro, 2 x t3.micro, 6 x t3.large, 10 x t3.small, 2 x t3.micro, 2 x t3.large, 2 x t3.medium, 10 x t3.large, 2 x t3.micro
        \item \textbf{20241011-1}: 4 x t3.micro, 10 x t3.micro, 8 x t3.micro, 4 x t3.large, 8 x t3.medium, 10 x t3.medium, 6 x t3.medium, 4 x t3.large, 4 x t3.medium, 6 x t3.medium
        \item \textbf{20298815-6}: 2 x t3.small, 10 x t3.medium, 10 x t3.large, 8 x t3.medium, 6 x t3.small, 6 x t3.medium, 2 x t3.micro, 2 x t3.small, 6 x t3.small, 2 x t3.large
        \item \textbf{20430363-0}: 4 x t3.micro, 4 x t3.small, 10 x t3.medium, 6 x t3.large, 8 x t3.medium, 10 x t3.micro, 4 x t3.small, 8 x t3.small, 10 x t3.large, 8 x t3.medium
        \item \textbf{20632334-5}: 6 x t3.medium, 4 x t3.medium, 4 x t3.medium, 6 x t3.large, 8 x t3.micro, 6 x t3.small, 10 x t3.large, 10 x t3.large, 4 x t3.small, 2 x t3.micro
        \item \textbf{20762701-1}: 8 x t3.micro, 2 x t3.medium, 6 x t3.micro, 10 x t3.small, 10 x t3.small, 10 x t3.medium, 4 x t3.large, 4 x t3.medium, 2 x t3.small, 2 x t3.large
        \item \textbf{20781979-4}: 10 x t3.large, 6 x t3.medium, 6 x t3.large, 2 x t3.small, 10 x t3.small, 2 x t3.medium, 10 x t3.small, 4 x t3.small, 8 x t3.medium, 8 x t3.large
        \item \textbf{20886083-6}: 2 x t3.large, 6 x t3.large, 4 x t3.micro, 8 x t3.micro, 4 x t3.large, 6 x t3.micro, 4 x t3.micro, 6 x t3.large, 6 x t3.large, 4 x t3.micro
        \item \textbf{20920259-K}: 4 x t3.small, 8 x t3.micro, 2 x t3.large, 8 x t3.medium, 6 x t3.micro, 8 x t3.small, 8 x t3.large, 2 x t3.small, 10 x t3.large, 4 x t3.micro
        \item \textbf{20966993-5}: 8 x t3.small, 6 x t3.small, 2 x t3.small, 10 x t3.large, 4 x t3.medium, 4 x t3.micro, 6 x t3.medium, 8 x t3.medium, 6 x t3.small, 8 x t3.micro
        \item \textbf{20969314-3}: 4 x t3.large, 10 x t3.small, 8 x t3.micro, 4 x t3.large, 4 x t3.medium, 10 x t3.micro, 10 x t3.micro, 8 x t3.small, 8 x t3.large, 4 x t3.large
        \item \textbf{21095788-K}: 4 x t3.micro, 4 x t3.small, 10 x t3.medium, 2 x t3.medium, 2 x t3.micro, 6 x t3.medium, 6 x t3.large, 10 x t3.small, 8 x t3.medium, 6 x t3.large
        \item \textbf{21127094-2}: 4 x t3.large, 6 x t3.small, 2 x t3.large, 2 x t3.small, 8 x t3.small, 2 x t3.medium, 10 x t3.medium, 2 x t3.small, 8 x t3.small, 10 x t3.medium
        \item \textbf{26799666-0}: 2 x t3.medium, 10 x t3.micro, 6 x t3.small, 2 x t3.medium, 10 x t3.micro, 2 x t3.medium, 6 x t3.small, 2 x t3.micro, 2 x t3.medium, 8 x t3.large
    \end{itemize}

    La cantidad de máquinas se refiere a la cantidad de máquinas trabajadoras. Usted debe desplegar el cluster para cada una de las configuraciones que le corresponden, ejecutar el test-000 (esto es necesario ya que este paso instala ciertas librerías que tienen un impacto relevante en el tiempo de ejecución) y luego medir el tiempo que demora el test-001. El tiempo debe ser reportado en \url{https://forms.gle/gGVc2wkqc6FzfyU99}. Debe completar el formulario para cada una de las 5 configuraciones solicitadas. En los casos de quienes tienen 2 veces asignada la misma configuración, se debe medir 2 veces y reportar 2 veces el tiempo requerido para ejecución.

    Con la información obtenida se debe completar la tabla \ref{table:002}. En la columna de \textbf{costo} debe calcular el costo que tuvo la ejecución en USD tomando como referencia los precios on-demand por hora disponibles en \url{https://aws.amazon.com/ec2/instance-types/t3/}.

    Los datos recopilados entre todos los estudiantes estarán disponibles en \url{https://docs.google.com/spreadsheets/d/1t53S2s0knpQnZl9EcZr2ZHWOg_qTCcNp6Kjpg7g1Wlo/edit?usp=sharing}. En base a esta información usted debe desarrollar una figura que permita apreciar el efecto multidimensional del escalamiento vertical y horizontal en el tiempo de ejecución del proceso. Debe considerar a lo menos 10 pares diferentes maquina/cantidad para poder desarrollar esta figura (deberá esperar a que exista esta cantidad mínima de información para poder proceder).
}
    \begin{table}
        \centering
        \begin{tabular}{|c|c|c|c|}
            \hline
            Maquinas & Tipo & Tiempo[s] & Costo[USD] \\
            \hline
            \hline
                     &      &           &            \\
            \hline
                     &      &           &            \\
            \hline
        \end{tabular}
        \caption{Tiempos de ejecución para medición de impacto de escalamiento}
        \label{table:002}
    \end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figure-000}
    \caption{Efecto del escalamiento vertical y horizontal en el tiempo de ejecución de test-001.sh
    {\color{red} Tiene libertad para elegir la metodología de visualización que le parezca más apropiada}}
    \label{fig:001}
\end{figure}

{\color{red} Discuta sobre como afecta el escalamiento horizontal y vertical el tiempo de proceso del test-001, considerando tanto a la información que recopiló con sus experimentos, la información obtenida desde los experimentos de los otros participantes de la asignatura, y el impacto de la dispersión de las mediciones.}

\section{Procesamiento de datos}

\subsection{Extracción, transformación y carga}

{\color{red} Desarrolle un programa basado en el test-001 que permita la extracción, transformación y carga (extraction, transformation and load, normalmente denominado ETL) del dataset de la asignatura. El set total de datos de la asignatura corresponde al archivo de 7.2[GB] disponible en \url{s3://utfsm-inf356-datasets/vlt_observations/vlt_observations.csv}. Este dataset ha sido dividido en 20 segmentos con nombre \url{s3://utfsm-inf356-datasets/vlt\_observations/vlt_observations_XXX.csv} para poder disponer de un acceso fraccionado a los datos.

    El proceso ETL debe procesar el dataset entregado y generar un nuevo dataset para procesamiento posterior, el que debe considerar las siguientes columnas:
    \begin{itemize}
        \item Right ascension - grados
        \item Right ascension - minutos
        \item Right ascension - segundos
        \item Declination - grados
        \item Declination - minutos
        \item Declination - segundos
        \item Instrument
        \item Exposition time
        \item Template start (en tiempo unix)
    \end{itemize}

    El dataset procesado sólo debe tener los registros que corresponden a las observaciones con categoría  SCIENCE y tipo de observación OBJECT (referencia \url{https://archive.eso.org/eso/eso_archive_help.html}).

    El resultado del dataset debe ser guardado en formato parquet en la raíz de su bucket de desarrollo bajo el nombre \textbf{vlt\_observations\_etl.parquet}. Se solicitará incluir el código bajo el nombre code-005.py en su entrega. Describa el procedimiento y muestre alguna métrica relativa a la ejecución y/o resultado de su procedimiento.}

\begin{code}[H]
    \lstinputlisting[style=pythonstyle, caption={Código utilizado para el procedimiento de ETL}, label={lst:005}]{code/code-005.py}
\end{code}

\subsection{Coordenadas galácticas}

{\color{red} Desarrolle un programa que lea el archivo generado por el proceso ETL y genere un nuevo archivo parquet en el que se han transformado las coordenadas RA-DEC ecuatoriales de las observaciones a coordenadas galácticas. El nuevo archivo debe estar en la raíz de su bucket de desarrollo y debe ser llamado \textbf{vlt\_observations\_gc.parquet}. El archivo debe tener las columnas:
    \begin{itemize}
        \item Galactic right ascension (float en grados)
        \item Galactic declination (float en grados)
        \item Instrument
        \item Exposition time
        \item Template start (en tiempo unix)
    \end{itemize}
    Se solicitará incluir el código bajo el nombre code-006.py en su entrega. Describa el procedimiento y muestre alguna métrica relativa a la ejecución y/o resultado de su procedimiento.}

\begin{code}[H]
    \lstinputlisting[style=pythonstyle, caption={Código utilizado para el cálculo de coordenadas galácticas}, label={lst:006}]{code/code-006.py}
\end{code}

\subsection{Segmentación temporal}

{\color{red} Segmente el archivo de coordenadas galácticas en tantos archivos como sea necesario de modo de contar con una agrupación por año y por semana del año. Se considera la primera semana del año la semana del primer lunes de un año. Los primeros días del año pertenecen al año anterior si ocurren antes del primer lunes del año.

    Los datos deben ser guardados en formato parquet en una carpeta llamada \textbf{partition} en la raiz de su bucket de desarrollo. Dentro de partition las carpetas se separarán por año. Dentro de la carpeta de un determinado año deberá haber un archivo denominado \textbf{vlt\_observations\_XXXX.parquet} con todos los datos del año, y una carpeta denominada weeks, que dentro debe tener una serie de archivos denominados \textbf{vlt\_observations\_XXXX\_YY.parquet}, donde XXXX corresponde al año y YY al número de semana comenzando en 0.

    Se solicitará incluir el código bajo el nombre code-007.py en su entrega. Describa el procedimiento y muestre alguna métrica relativa a la ejecución y/o resultado de su procedimiento.}

\begin{code}[H]
    \lstinputlisting[style=pythonstyle, caption={Código utilizado para segmentación temporal de datos}, label={lst:007}]{code/code-007.py}
\end{code}

\subsection{Conteo de observaciones}

{\color{red} Desarrolle un programa que reciba un archivo parquet con el formato previamente utilizado para las coordenadas galácticas y que genere un conteo de las observaciones para cada segmento de 10 grados horizontal y vertical del cielo, segmentado por cada instrumento. El archivo de salida debe tener el mismo nombre que el archivo de entrada pero con un .count antes de la extensión .parquet. El conteo tiene que tener como coordenada de referencia el centro de la región angular. El tiempo de exposición debe ser reemplazado por el tiempo total de observaciones que han sido considerados en la cuenta. Se debe descartar la columna con el tiempo de inicio del template.

    Se solicitará incluir el código bajo el nombre code-008.py en su entrega. Describa el procedimiento y muestre alguna métrica relativa a la ejecución y/o resultado de su procedimiento.}

\begin{code}[H]
    \lstinputlisting[style=pythonstyle, caption={Código utilizado para segmentación temporal de datos}, label={lst:007}]{code/code-008.py}
\end{code}

\end{document}
